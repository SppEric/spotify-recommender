{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591fe09b-fa84-47b8-999c-511cd678fea4",
   "metadata": {
    "id": "591fe09b-fa84-47b8-999c-511cd678fea4"
   },
   "source": [
    "# Long Short-Term Memory Networks\n",
    "\n",
    "In this additional challenge, students will build their own LSTM layer from scratch.  \n",
    "***This one is optional for 1470 students, but can be done for bonus credit!***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77d1bb-cea8-4532-b4c7-392f6be47723",
   "metadata": {
    "executionInfo": {
     "elapsed": 7536,
     "status": "ok",
     "timestamp": 1656624692044,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "bc77d1bb-cea8-4532-b4c7-392f6be47723"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from preprocess import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7b3e2-4af8-4e5e-9f66-c582e5ae9d26",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656624692045,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "dce7b3e2-4af8-4e5e-9f66-c582e5ae9d26"
   },
   "outputs": [],
   "source": [
    "data_path = \"../../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e212c2-fc87-4de7-b15d-0b28225b1b83",
   "metadata": {
    "id": "55e212c2-fc87-4de7-b15d-0b28225b1b83"
   },
   "source": [
    "## Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acafb39f-917e-4a94-a9be-df97259cde4f",
   "metadata": {
    "id": "acafb39f-917e-4a94-a9be-df97259cde4f"
   },
   "source": [
    "No spoilers for the preprocessing part of the homework here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9e566-1b58-4d6d-ae6c-cc53b003fabc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1656624692188,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "a1e9e566-1b58-4d6d-ae6c-cc53b003fabc",
    "outputId": "c3a5a0b5-d17c-4d7a-9e95-b5c29e62fa3c"
   },
   "outputs": [],
   "source": [
    "example_sentence = \"The word <_UNK> is not a common word but flower is a common word\"\n",
    "\n",
    "example_sentence_list = [\n",
    "    'the', 'word', '<_unk>', 'is', 'not', \n",
    "    'a', 'common', 'word', 'but', 'flower', \n",
    "    'is', 'a', 'common', 'word']\n",
    "example_unique_words = [\n",
    "    '<_unk>', 'a', 'but', 'common', \n",
    "    'flower', 'is', 'not', 'the', 'word']\n",
    "example_w2t_dict = {\n",
    "    '<_unk>': 0, 'a': 1, 'but': 2, 'common': 3, 'flower': 4, \n",
    "    'is': 5, 'not': 6, 'the': 7, 'word': 8}\n",
    "example_sentence_tokenized = [7, 8, 0, 5, 6, 1, 3, 8, 2, 4, 5, 1, 3, 8]\n",
    "\n",
    "print(f\"1. example_sentence_list \\n    {example_sentence_list}\\n\")\n",
    "print(f\"2. example_unique_words \\n    {example_unique_words}\\n\")\n",
    "print(f\"3. example_w2t_dict \\n    {example_w2t_dict}\\n\")\n",
    "print(f\"4. example_sentence_tokenized \\n    {example_sentence_tokenized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ed85e-ca14-4e48-a46d-e5b2ebd4e286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1656624692188,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "662ed85e-ca14-4e48-a46d-e5b2ebd4e286",
    "outputId": "d2e7168b-31e4-467e-9eea-dd94b6e75576"
   },
   "outputs": [],
   "source": [
    "X_RNN = np.array([\n",
    "    [7, 8, 0, 5],\n",
    "    [6, 1, 3, 8],\n",
    "    [2, 4, 5, 1]])\n",
    "y_RNN = np.array([\n",
    "    [8, 0, 5, 6],\n",
    "    [1, 3, 8, 2],\n",
    "    [4, 5, 1, 3]])\n",
    "\n",
    "print(f\"X_RNN shape = {X_RNN.shape}\")\n",
    "print(f\"y_RNN shape = {y_RNN.shape}\")\n",
    "\n",
    "print(f\"X_RNN     --> y_RNN\")\n",
    "for each_X, each_y in zip(X_RNN, y_RNN):\n",
    "    print(f\"{each_X} --> {each_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edefc96-2073-4618-b3e7-b792f0615b0e",
   "metadata": {
    "id": "9edefc96-2073-4618-b3e7-b792f0615b0e"
   },
   "source": [
    "## Keras LSTM Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba1ad7-6def-40e9-8ab6-ea5dcc45ef67",
   "metadata": {
    "id": "d8ba1ad7-6def-40e9-8ab6-ea5dcc45ef67"
   },
   "source": [
    "We've already looked at `tf.keras.layers.LSTM`'s API. \n",
    "\n",
    "- The Keras LSTM Layer expects the input shape to be in the **batch-major form**, `[batch, timesteps, embedding]`. \n",
    "- In our language model, `timesteps` is basically our `window`. \n",
    "  - That's because we treat a sequence of words as a time-series data.\n",
    "\n",
    "Also, the most important keywards arguments are `units`, `return_state` and `return_sequences`. \n",
    "- `units` is the output embedding size, \n",
    "- `return_state` and `return_sequences` are the Boolean variables to return the final state and the sequences of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4cbb5-5635-4632-8365-228a60f95241",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1656624692402,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "a5b4cbb5-5635-4632-8365-228a60f95241",
    "outputId": "88b246ff-39a0-4d98-e405-0b419e25e5e9"
   },
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(input_dim  = 9, \n",
    "                                            output_dim = 2)\n",
    "\n",
    "X_RNN_embedding = embedding_layer(X_RNN)\n",
    "batch_size, window_size, embedding_size= X_RNN_embedding.shape ## (3, 4, 2)\n",
    "print(f\"RNN input tokens shape = {X_RNN.shape}\")\n",
    "print(f\"RNN embeddings shape   = {X_RNN_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a12a3-4e30-491f-baf1-1972d9703568",
   "metadata": {
    "id": "fc7a12a3-4e30-491f-baf1-1972d9703568"
   },
   "source": [
    "We also know that all Keras LSTM layers have the same weight structures, no matter the value of the Boolean flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b5e72-9d07-4c42-859d-cd6a38cdb973",
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1656624692558,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "610b5e72-9d07-4c42-859d-cd6a38cdb973"
   },
   "outputs": [],
   "source": [
    "lstm           = tf.keras.layers.LSTM(units=embedding_size, return_sequences=False, return_state=False)\n",
    "lstm_state     = tf.keras.layers.LSTM(units=embedding_size, return_sequences=False, return_state=True )\n",
    "lstm_seq       = tf.keras.layers.LSTM(units=embedding_size, return_sequences=True,  return_state=False)\n",
    "lstm_seq_state = tf.keras.layers.LSTM(units=embedding_size, return_sequences=True,  return_state=True )\n",
    "\n",
    "lstm.build(X_RNN_embedding.shape)\n",
    "lstm_state.build(X_RNN_embedding.shape)\n",
    "lstm_seq.build(X_RNN_embedding.shape)\n",
    "lstm_seq_state.build(X_RNN_embedding.shape)\n",
    "\n",
    "lstm_weights = lstm.get_weights()\n",
    "lstm_state.set_weights(lstm_weights)\n",
    "lstm_seq.set_weights(lstm_weights)\n",
    "lstm_seq_state.set_weights(lstm_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd4a1e-ef5b-431d-8918-0d8bd555ac87",
   "metadata": {
    "id": "91dd4a1e-ef5b-431d-8918-0d8bd555ac87"
   },
   "source": [
    "### Keras LSTM Layer Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ac98a-519e-48b6-94dc-c5ee6d6c40be",
   "metadata": {
    "id": "307ac98a-519e-48b6-94dc-c5ee6d6c40be"
   },
   "source": [
    "It's time to see how those weights work under the hood. \n",
    "- The LSTM weights are in fact three trainable Tensor variables named `kernel`, `recurrent_kernel`. and `bias`.\n",
    "- `kernel` is the array of weights for the input\n",
    "- `recurrent_kernel` is the array of weights for the previous hidden state\n",
    "- `bias` is the array of biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9114279-f78b-450e-8348-fe4c48a50628",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1656624692692,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "b9114279-f78b-450e-8348-fe4c48a50628",
    "outputId": "2f44d17d-348b-4744-b952-d0c9d82b9609"
   },
   "outputs": [],
   "source": [
    "for each_weight_tensor in lstm_seq_state.weights:\n",
    "    print(each_weight_tensor.name)\n",
    "    print(each_weight_tensor, end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52aa3a-9467-4693-bf71-1df1bb731a52",
   "metadata": {
    "id": "2c52aa3a-9467-4693-bf71-1df1bb731a52"
   },
   "source": [
    "At this point, you might be wondering \n",
    "> but wait a second. Shouldn't there be **four pairs of weights and biases**, <br>\n",
    "> because there are four internal feed-forward netwroks in a LSTM unit?\n",
    "\n",
    "And, you are right. There are four pairs of weights and biases for each internal feed-forward network, but the developers of TensorFlow and Keras only decided to put the weights and biases together in a different way. We can reshape them to be make it easier for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238df20-13a5-4602-af0c-b2c666710210",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1656624692692,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "a238df20-13a5-4602-af0c-b2c666710210"
   },
   "outputs": [],
   "source": [
    "units = embedding_size\n",
    "W, U, b = lstm_weights\n",
    "\n",
    "### kernel: weights for the input vector x_{t}\n",
    "W_i, W_f, W_c, W_o = (\n",
    "    W[:, :units], W[:, units:(2*units)], W[:, (2*units):(3*units)], W[:, (3*units):])\n",
    "\n",
    "### recurrent kernel: weights for the previous hidden state h_{t-1}\n",
    "U_i, U_f, U_c, U_o = (\n",
    "    U[:, :units], U[:, units:(2*units)], U[:, (2*units):(3*units)], U[:, (3*units):])\n",
    "\n",
    "### bias\n",
    "b_i, b_f, b_c, b_o = (\n",
    "    b[:units], b[units:(units*2)], b[(units*2):(units*3)], b[(units*3):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00192d2-85cb-4a2f-8aee-f6c54e38b6fd",
   "metadata": {
    "id": "a00192d2-85cb-4a2f-8aee-f6c54e38b6fd"
   },
   "source": [
    "## Your Own Implementation of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67180dbe-e8d2-43cc-a728-c3b8e8235980",
   "metadata": {
    "id": "67180dbe-e8d2-43cc-a728-c3b8e8235980"
   },
   "source": [
    "Now we can use the weights and biases $W$, $U$, and $b$ in the way that we've covered in the lecture. \n",
    "\n",
    "- $x_t$ is the current input at timestep $t$.\n",
    "- $h_{t-1}$ and $c_{t-1}$ are the previous hidden and cell states. \n",
    "\n",
    "\\begin{align*}\n",
    "f_t &= \\sigma \\left( W_f x_t + U_f h_{t-1} + b_f \\right) & \\textsf{Forget Module}\\\\\n",
    "i_t &= \\sigma \\left( W_i x_t + U_i h_{t-1} + b_i \\right) & \\textsf{Remember Module}\\\\\n",
    "\\tilde{c}_t &= \\tanh \\left( W_c x_t + U_c h_{t-1} + b_c \\right) & \\textsf{New Memory}\\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t  & \\textsf{Cell State Update}\\\\\n",
    "o_t &= \\sigma \\left( W_o x_t + U_o h_{t-1} + b_o \\right) & \\textsf{Output Module}\\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t)  & \\textsf{Output, Hidden State Update}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, your job is to finish implementing the `call` method below. \n",
    "- The inputs need to be reshaped into the time-major form `[timesteps, batch, embedding]`.\n",
    "  - This is because is parallelizing the recurrent operations through all timesteps is very difficult.\n",
    "  - So, we will use the for-loop to advance in the timestep dimension.\n",
    "  - Then, inside the for-loop, we wil use matrix multiplications for the batch of inputs in the same timestep.\n",
    "- Remember that, in a single timestep, the input data is in the matrix form with shape `[batch, embedding]`.\n",
    "  - So, do something like $Y = XW$ instead of $y_i = W x_i, i \\in \\{1, 2, 3, \\cdots\\}$.\n",
    "  - Also, the hidden and cell states are also matrices with the same shape `[batch, embedding]`.\n",
    "- You should return the whole sequence of outputs and the final hidden and cell states\n",
    "  - Like when `return_sequences = True` and `return_state = True`.\n",
    "- The outputs needs to be reshaped back into the batch-major form `[batch, timesteps, embedding]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a705d4-2900-4c3c-99b7-41d82f20ea5a",
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1656624692811,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "45a705d4-2900-4c3c-99b7-41d82f20ea5a"
   },
   "outputs": [],
   "source": [
    "class MyLSTM(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        super(MyLSTM, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        kernel_shape = tf.TensorShape((input_shape[-1], 4*self.units))\n",
    "\n",
    "        # Create trainable weight variables for this layer.\n",
    "        self.kernel = self.add_weight(\n",
    "            name = \"kernel\", \n",
    "            shape = kernel_shape,\n",
    "            dtype = tf.float32,\n",
    "            initializer= \"glorot_uniform\",\n",
    "            trainable = True)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            name = \"recurrent_kernel\",\n",
    "            shape = kernel_shape,\n",
    "            dtype = tf.float32,\n",
    "            initializer = \"orthogonal\",\n",
    "            trainable = True)\n",
    "        self.bias = self.add_weight(\n",
    "            name = \"bias\",\n",
    "            shape = (4*self.units,),\n",
    "            dtype = tf.float32,\n",
    "            initializer = \"zeros\",\n",
    "            trainable = True)\n",
    "        \n",
    "        # Make sure to call the `build` method at the end\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, initial_state = None):\n",
    "\n",
    "        ## TODO: Implement LSTM internals\n",
    "\n",
    "        ## Hidden state and cell state\n",
    "        if initial_state:\n",
    "            ht, ct = tf.identity(initial_state[0]), tf.identity(initial_state[1])\n",
    "        else:\n",
    "            ht = tf.zeros(shape=(inputs.shape[0], self.units), dtype=tf.float32)\n",
    "            ct = tf.zeros(shape=(inputs.shape[0], self.units), dtype=tf.float32)\n",
    "        \n",
    "        outputs = [] ## we need the whole sequence of outputs\n",
    "        inputs_time_major = tf.transpose(inputs, perm = [1, 0, 2]) ## swap the batch and timestep axes\n",
    "        \n",
    "        return outputs, ht, ct\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.units\n",
    "        return tf.TensorShape(shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(MyLSTM, self).get_config()\n",
    "        base_config[\"units\"]   = self.units\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca63082-2d07-4905-ac5b-cd49d7acb1ef",
   "metadata": {
    "id": "0ca63082-2d07-4905-ac5b-cd49d7acb1ef"
   },
   "source": [
    "## Compare with Keras LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3dfaea-0926-4674-a56b-7f27b76694c4",
   "metadata": {
    "id": "2c3dfaea-0926-4674-a56b-7f27b76694c4"
   },
   "source": [
    "Now we have to see if your LSTM layer returns the exact same outputs as the Keras LSTM layer. \n",
    "\n",
    "So, we will initialize your LSTM layer with **the same weights** from the other Keras LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82049d9e-adc7-4d89-9953-62770aea2fb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1656624692925,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "82049d9e-adc7-4d89-9953-62770aea2fb3",
    "outputId": "2f645480-0e16-4bed-e6a4-eff1209a26eb"
   },
   "outputs": [],
   "source": [
    "my_lstm = MyLSTM(units = 2)\n",
    "my_lstm.build(X_RNN_embedding.shape)\n",
    "my_lstm.set_weights(lstm_weights)\n",
    "my_lstm.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07940745-c6d3-480e-84bd-55ef5f1c7ed5",
   "metadata": {
    "id": "07940745-c6d3-480e-84bd-55ef5f1c7ed5"
   },
   "source": [
    "Then calculate the outputs and the states from your own LSTM layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69479605-3300-49a6-b2c7-4fc5f1cf1611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1656624693213,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "69479605-3300-49a6-b2c7-4fc5f1cf1611",
    "outputId": "9faed304-7095-4e59-ebfe-6ee9500c93fc"
   },
   "outputs": [],
   "source": [
    "# random initial states\n",
    "tf_generator = tf.random.Generator.from_seed(42)\n",
    "input_state_h = tf_generator.normal(shape=(1, units))\n",
    "input_state_c = tf_generator.normal(shape=(1, units))\n",
    "\n",
    "# initial states in the [batch, embedding] format\n",
    "ht = tf.repeat(input_state_h, repeats=batch_size, axis=0)\n",
    "ct = tf.repeat(input_state_c, repeats=batch_size, axis=0)\n",
    "\n",
    "my_output_seq, my_state_h, my_state_c = my_lstm(X_RNN_embedding, initial_state = (ht, ct))\n",
    "print(f\"my output sequence, shape = {my_output_seq.shape} \\n{my_output_seq}\\n\")\n",
    "print(f\"my final hidden state, shape = {my_state_h.shape} \\n{my_state_h}\\n\")\n",
    "print(f\"my final cell state, shape = {my_state_c.shape} \\n{my_state_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedef693-7028-48ea-bdc7-f3ddfa7e36e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1656624693370,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "fedef693-7028-48ea-bdc7-f3ddfa7e36e6",
    "outputId": "016b56fb-4748-4876-9cfd-2d950edfef25"
   },
   "outputs": [],
   "source": [
    "keras_output_seq, keras_state_h, keras_state_c = lstm_seq_state(X_RNN_embedding, initial_state = (ht, ct))\n",
    "\n",
    "print(f\"Keras output sequence, shape = {keras_output_seq.shape} \\n{keras_output_seq}\\n\")\n",
    "print(f\"Keras final hidden state, shape = {keras_state_h.shape} \\n{keras_state_h}\\n\")\n",
    "print(f\"Keras final cell state, shape = {keras_state_c.shape} \\n{keras_state_c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990cc4f5-91c2-459b-bb33-23de5f0b0c40",
   "metadata": {
    "id": "990cc4f5-91c2-459b-bb33-23de5f0b0c40"
   },
   "source": [
    "If you have implemented your LSTM layer correctly, you will have the same outputs and states within reasonable error margins for the floating point representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4596d-c2c2-4e57-a51d-4efa384215ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1656624693370,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "b2d4596d-c2c2-4e57-a51d-4efa384215ab",
    "outputId": "c8900254-bde0-475e-d26a-cfd1279037a0"
   },
   "outputs": [],
   "source": [
    "print(np.allclose(my_output_seq.numpy(), keras_output_seq.numpy()))\n",
    "print(np.allclose(my_state_h.numpy(), keras_state_h.numpy()))\n",
    "print(np.allclose(my_state_c.numpy(), keras_state_c.numpy()))\n",
    "# use np.isclose for element-wise comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a95b9-8e1c-49c5-8497-ee561ffb299f",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656624693371,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "ac9a95b9-8e1c-49c5-8497-ee561ffb299f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7702b88cad6d75597f6011cf92cdcfa745d6308cad626581b1fa8c05db9a3bbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
