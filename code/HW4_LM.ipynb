{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e854ba-1f4e-4e24-9808-10835720210b",
   "metadata": {
    "id": "01e854ba-1f4e-4e24-9808-10835720210b"
   },
   "source": [
    "# CS1470/2470 HW4: Language Models\n",
    "\n",
    "In this homework assignment, you will build deep learning language models. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5867603-2b77-47ea-8cdb-97fa84e1a491",
   "metadata": {
    "id": "e5867603-2b77-47ea-8cdb-97fa84e1a491"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import   preprocess, trigram, rnn\n",
    "#%aimport preprocessing.py#, trigram, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa7e7a-c467-4f48-9fc6-2a7d28c35eda",
   "metadata": {
    "id": "1aaa7e7a-c467-4f48-9fc6-2a7d28c35eda"
   },
   "outputs": [],
   "source": [
    "data_path = \"../data\" ## TODO: Maybe edit if need be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371287d3-2c38-472e-b04b-237b2aa24eb6",
   "metadata": {
    "id": "371287d3-2c38-472e-b04b-237b2aa24eb6"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3535efa-e1e4-47fe-8969-79d0d3c8dc6f",
   "metadata": {
    "id": "a3535efa-e1e4-47fe-8969-79d0d3c8dc6f"
   },
   "source": [
    "When you open the provided text files, `train.txt` and `test.txt`, you will see that the raw corpus has already been through some preprocessing. For example, some uncommon words were replaced with `<_UNK>`, such as in the following sentence \n",
    "\n",
    "> The word \"photosysnthesis\" is not a common word, but \"flower\" is a common word.\n",
    "\n",
    "when has been preprocessed into:\n",
    "\n",
    "```\n",
    "The word \" <_UNK> \" is not a common word , but \" flower \" is a common word .\n",
    "```\n",
    "\n",
    "This practice of \"unk-ing\" is done to make the neural network learn as few words as possible while still being able to preserve most of the original meaning in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cb81f-059c-427f-8f26-88eca6be659a",
   "metadata": {
    "id": "580cb81f-059c-427f-8f26-88eca6be659a"
   },
   "source": [
    "### Preprocess Warm-up Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b969b-f016-45d1-9143-6535982a59e7",
   "metadata": {
    "id": "874b969b-f016-45d1-9143-6535982a59e7"
   },
   "source": [
    "Now your job is to implment the `get_data` function in `preprocess.py`. However, before you begin, let's do a small practice here as a warm-up. In this practice, we will pretend that the sentence \"The word <\\_UNK> is not a commonly used word but flower is\" is the entirety of your corpus. The punctuation marks are ignored just to keep this exercise simple.\n",
    "\n",
    "Here are what we are going to do.\n",
    "1. Convert the sentence into a concatenated list of every word in that sentence.\n",
    "   - ```['the', 'word', '<_unk>', 'is', 'not', 'a', 'common', 'word', 'but', 'flower', 'is', 'a', 'common', 'word']```\n",
    "   - Lower the case of all words.\n",
    "2. Create a list of all unique words that appears in the sentence.\n",
    "   + ```['<_unk>', 'a', 'but', 'common', 'flower', 'is', 'not', 'the', 'word']```\n",
    "   + The unique words do not have to be sorted to train the neural network\n",
    "   + but let's have them sorted in the alphabetical order to make sure that we all have the exact same mapping. \n",
    "3. Create a dictionary that maps each word to its own index in the list of unique words.\n",
    "   - ```{'<_unk>': 0, 'a': 1, 'but': 2, 'common': 3, 'flower': 4, 'is': 5, 'not': 6, 'the': 7, 'word': 8}```\n",
    "   - The index numbers will be used as tokens.\n",
    "4. Convert the sentence into a list of tokens.\n",
    "   + ```[7, 8, 0, 5, 6, 1, 3, 8, 2, 4, 5, 1, 3, 8]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d4b5f-2739-4e68-a6e7-825e13a1109f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1656613549539,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "992d4b5f-2739-4e68-a6e7-825e13a1109f",
    "outputId": "6bb11e36-5b86-4d7f-9bbe-e6e0a978b2a6"
   },
   "outputs": [],
   "source": [
    "example_sentence = \"The word <_UNK> is not a common word but flower is a common word\"\n",
    "\n",
    "## Answer Key\n",
    "example_sentence_list = example_sentence.lower().split()\n",
    "example_unique_words = sorted(set(example_sentence_list))\n",
    "example_w2t_dict = {w:i for i, w in enumerate(example_unique_words)}\n",
    "example_sentence_tokenized = [example_w2t_dict[w] for w in example_sentence_list]\n",
    "\n",
    "print(f\"1. example_sentence_list \\n    {example_sentence_list}\\n\")\n",
    "print(f\"2. example_unique_words \\n    {example_unique_words}\\n\")\n",
    "print(f\"3. example_w2t_dict \\n    {example_w2t_dict}\\n\")\n",
    "print(f\"4. example_sentence_tokenized \\n    {example_sentence_tokenized}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569fa65-569c-42cc-baaa-19cc3efb3a4f",
   "metadata": {
    "id": "8569fa65-569c-42cc-baaa-19cc3efb3a4f"
   },
   "source": [
    "### Tokenized Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ca28f-3117-4983-9db8-8a141e773af7",
   "metadata": {
    "id": "ac7ca28f-3117-4983-9db8-8a141e773af7"
   },
   "source": [
    "It is now the time for you to finish the `get_data` function in the file `preprocess.py`. Then come back to this notebook and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517f466-0b7a-4fc5-b709-dbd5eddf0fe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2459,
     "status": "ok",
     "timestamp": 1656613551989,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "7517f466-0b7a-4fc5-b709-dbd5eddf0fe2",
    "outputId": "19aaec91-dcdd-46e5-abdf-b519b6df02eb"
   },
   "outputs": [],
   "source": [
    "## get the tokenized list of words from the corpus\n",
    "train_tracks_id, test_tracks_id, track_to_id, relevance_output = preprocessing.preprocess()\n",
    "\n",
    "## A useful utility for counting things\n",
    "word_counter = collections.Counter(train_tracks_id)\n",
    "\n",
    "## What are the 40 most common words?\n",
    "n_most_common = 40\n",
    "most_common_tokens, most_common_occurrences = zip(*word_counter.most_common(n_most_common))\n",
    "\n",
    "## Convert the tokens back to words so that we can see what they are\n",
    "token_to_word_dict = {i:w for w, i in track_to_id.items()}\n",
    "most_common_words = [token_to_word_dict[t] for t in most_common_tokens]\n",
    "\n",
    "print(*zip(most_common_words, most_common_occurrences), sep = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714a111-c26b-434e-9c6c-ae63b04d2a12",
   "metadata": {
    "id": "9714a111-c26b-434e-9c6c-ae63b04d2a12"
   },
   "source": [
    "We should have the exact same list of the most common words and their occurrences in the training set.\n",
    "```\n",
    "('the', 85323), ('_s', 84244), ('.', 81647), (',', 51885), ('<UNK>', 45809), ('<NUM>', 44275), ('of', 40101), ('_ed', 35532), ('in', 34150), ('a', 28963), ('is', 28070), ('and', 24940), (':', 23194), ('to', 17763), ('was', 17231), ('it', 13960), ('<STOP>', 13444), ('-', 13055), ('\"', 11112), ('_ing', 10678), ('on', 9371), ('for', 9141), ('are', 7767), ('as', 7250), ('have', 7203), ('by', 7170), ('he', 6619), ('that', 6565), ('from', 6203), ('county', 5818), ('an', 5478), ('or', 5122), ('-PRON-', 5077), ('at', 4717), ('they', 4416), ('with', 4220), ('people', 4147), ('united', 4130), ('be', 3802), ('this', 3618)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34c845-5dc5-483c-9167-496154cef5ff",
   "metadata": {
    "id": "fe34c845-5dc5-483c-9167-496154cef5ff"
   },
   "source": [
    "Here is a histogram for you. Notice how the number of occurrences decreases exponentially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c58056-62d8-49ff-b046-49c12fb69062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1656613552588,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "58c58056-62d8-49ff-b046-49c12fb69062",
    "outputId": "6d940990-de75-40c1-c4ad-5faf56fa0b35"
   },
   "outputs": [],
   "source": [
    "fig_most_common, ax_top50_most_common = plt.subplots()\n",
    "ax_top50_most_common.barh(y = most_common_words,\n",
    "                          width = most_common_occurrences, \n",
    "                          height = 0.75, \n",
    "                          color = \"C0\", \n",
    "                          edgecolor = \"black\", \n",
    "                          zorder = 100)\n",
    "\n",
    "ax_top50_most_common.grid(linestyle = \"dashed\", \n",
    "                          color = \"#bfbfbf\", \n",
    "                          zorder = -100)\n",
    "\n",
    "ax_top50_most_common.set_yticks(ticks = ax_top50_most_common.get_yticks())\n",
    "ax_top50_most_common.set_yticklabels(labels = most_common_words, \n",
    "                                     fontsize = 14)\n",
    "\n",
    "ax_top50_most_common.invert_yaxis()\n",
    "## If you want log-scale \n",
    "# ax_top50_most_common.set_xscale('log')\n",
    "\n",
    "fig_most_common.set_size_inches([6, 12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121c258-6c96-4f79-ae3e-b5f55fd83c82",
   "metadata": {
    "id": "9121c258-6c96-4f79-ae3e-b5f55fd83c82"
   },
   "source": [
    "## Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441d065-2977-498a-b49e-f3b2b5c2322b",
   "metadata": {
    "id": "c441d065-2977-498a-b49e-f3b2b5c2322b"
   },
   "source": [
    "### Trigram Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd7567-cd88-4333-ad00-ba552e017335",
   "metadata": {
    "id": "aebd7567-cd88-4333-ad00-ba552e017335"
   },
   "source": [
    "We want the input `X` and the output `y` to be like this\n",
    "```\n",
    "X = [['the', 'word'], ['word', '<_unk>'], ['<_unk>', 'is'], ['is', 'not'], \n",
    "     ['not', 'a'], ['a', 'common'], ['common', 'word'], ['word', 'but'], \n",
    "     ['but', 'flower'], ['flower', 'is'], ['is', 'a'], ['a', 'common']`]\n",
    "y = ['<_unk>', 'is', 'not', 'a', \n",
    "     'common', 'word', 'but', 'flower', \n",
    "     'is', 'a', 'common', 'word']`\n",
    "```\n",
    "\n",
    "In this way, we can train the model to predict a word from by looking at its two previous words.\n",
    "```\n",
    "['the', 'word']    --> '<_unk>'\n",
    "['word', '<_unk>'] --> 'is'\n",
    "['<_unk>', 'is']   --> 'not'\n",
    "['is', 'not']      --> 'a'\n",
    "['not', 'a']       --> 'common'\n",
    "['a', 'common']    --> 'word'\n",
    "['common', 'word'] --> 'but'\n",
    "['word', 'but']    --> 'flower'\n",
    "['but', 'flower']  --> 'is'\n",
    "['flower', 'is']   --> 'a'\n",
    "['is', 'a']        --> 'common'\n",
    "['a', 'common']    --> 'word'\n",
    "```\n",
    "\n",
    "Of course, the actual input and output data has to be the tokens of the the words and not the words themselves. Construct the input and output data `X_trigram` and `y_trigram` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7951e-ffb4-4404-ae79-1d76c17cfff5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1656613552589,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "08c7951e-ffb4-4404-ae79-1d76c17cfff5",
    "outputId": "e0362b95-2e05-4ea7-c86a-c8467dfe4d33"
   },
   "outputs": [],
   "source": [
    "# print(f\"token list = {example_sentence_tokenized}\")\n",
    "\n",
    "# example_sentence_tokenized_array = np.array(example_sentence_tokenized)\n",
    "\n",
    "# X_trigram = np.vstack([example_sentence_tokenized_array[0:-2],\n",
    "#                        example_sentence_tokenized_array[1:-1]]).T\n",
    "\n",
    "# y_trigram = example_sentence_tokenized_array[2:]\n",
    "\n",
    "# print(f\"X_trigram shape = {X_trigram.shape}\")\n",
    "# print(f\"y_trigram shape = {y_trigram.shape}\")\n",
    "\n",
    "# print(f\"X_trigram --> y_trigram\")\n",
    "# for each_X, each_y in zip(X_trigram, y_trigram):\n",
    "#     print(f\"   {each_X}  -->  {each_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674df42-2d4b-4c21-9e8b-edbd25646f21",
   "metadata": {
    "id": "b674df42-2d4b-4c21-9e8b-edbd25646f21"
   },
   "source": [
    "### Embedding Look-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8617c-92dd-4933-b5e4-d357dc044c4d",
   "metadata": {
    "id": "a2d8617c-92dd-4933-b5e4-d357dc044c4d"
   },
   "source": [
    "Right now, we have exactly 9 unique words in our example sentence, and they are `['<_unk>', 'a', 'but', 'common', 'flower', 'is', 'not', 'the', 'word']`. Now the goal is to map each unique word to the corresponding vector in the embedding space. In this little practice, let's use a 2-dimensional embedding space and map each word to the very simple looking vectors in the table below. \n",
    "\n",
    "| word  |token|   vector   |\n",
    "|-------|-----|------------|\n",
    "|<_unk> |  0  | [0.0, 0.0] |\n",
    "|a      |  1  | [0.1, 0.2] |\n",
    "|but    |  2  | [0.2, 0.4] |\n",
    "|common |  3  | [0.3, 0.6] |\n",
    "|flower |  4  | [0.4, 0.8] |\n",
    "|is     |  5  | [0.5, 1.0] |\n",
    "|not    |  6  | [0.6, 1.2] |\n",
    "|the    |  7  | [0.7, 1.4] |\n",
    "|word   |  8  | [0.8, 1.6] |\n",
    "\n",
    "This is what we call an **embedding table** and please remember that what we have here is just a toy example, and the embedding vectors have silly values only for the convenience of illustration. \n",
    "\n",
    "In practice:\n",
    "- The embedding space needs far more dimenions than just two.\n",
    "- The embedding vectors should reflect some kind of semantic meaning and grammatical role of each word.\n",
    "\n",
    "Then, you can have a 2D array like `embedding_table` in the cell below to represent the embedding table, so that we can look up each word from the table with a function something like `lookup` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75efaf-0a5c-4edb-91d0-aea8793591af",
   "metadata": {
    "id": "bc75efaf-0a5c-4edb-91d0-aea8793591af"
   },
   "outputs": [],
   "source": [
    "embedding_table = np.array([\n",
    "    [0.0, 0.0], [0.1, 0.2], [0.2, 0.4], [0.3, 0.6], [0.4, 0.8], \n",
    "    [0.5, 1.0], [0.6, 1.2], [0.7, 1.4], [0.8, 1.6]])\n",
    "\n",
    "def lookup(tokens, embedding_table):\n",
    "    embedding_vectors = embedding_table[tokens]\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1b773-1052-419a-b843-4ce93f582844",
   "metadata": {
    "id": "fee1b773-1052-419a-b843-4ce93f582844"
   },
   "source": [
    "For example, the words `[\"flower\", \"is\"]`are mapped into their tokens `[4, 5]`. We know from the table that the embedding for `\"flower\"` is `[0.4, 0.8]` and the embedding for `\"is\"` is `[0.5, 1.0]`, and that's exactly what the `lookup` function returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54328303-bf2c-4253-ae63-ecc113664bcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656613552589,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "54328303-bf2c-4253-ae63-ecc113664bcd",
    "outputId": "d04c0922-8144-4fcf-93f3-2b6153708aaf"
   },
   "outputs": [],
   "source": [
    "tokens = np.array([4, 5])\n",
    "embedding_vectors = lookup(tokens, embedding_table)\n",
    "print(f\"tokens \\n{tokens} \\n \\nembedding vectors \\n{embedding_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58db22-571e-4158-a8c7-eb2529219c67",
   "metadata": {
    "id": "ba58db22-571e-4158-a8c7-eb2529219c67"
   },
   "source": [
    "We can even batch the input tokens. For example, if you want to look up `[\"flower\", \"is\"]` and `[\"not\", \"common\"]` at once, we can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1f7e2-d15e-4f52-8072-d45f8f60b9db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656613552589,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "42b1f7e2-d15e-4f52-8072-d45f8f60b9db",
    "outputId": "9840c75e-1333-4a37-b9c2-c2b590fc95d3"
   },
   "outputs": [],
   "source": [
    "tokens_batch = np.array([[4, 5], [6, 3]])\n",
    "embedding_vectors_batch = lookup(tokens_batch, embedding_table)\n",
    "print(f\"tokens \\n{tokens_batch} \\n \\nembedding vectors \\n{embedding_vectors_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54ddb3-daa2-4029-9e69-c8ee8824efda",
   "metadata": {
    "id": "da54ddb3-daa2-4029-9e69-c8ee8824efda"
   },
   "source": [
    "A fully connected dense layer expects a batch of flattened vectors, which is not a big problem. We can simply reshape the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a457231-27cd-49b7-94a2-760df908c2ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656613552590,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "5a457231-27cd-49b7-94a2-760df908c2ab",
    "outputId": "056d1d7c-3ba1-47f1-b045-c3d5251c5f62"
   },
   "outputs": [],
   "source": [
    "embedding_vectors_batch.reshape(2, 2*2) #(batch size, words in single input*dimensions for each word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2a31a-0e26-4747-aec2-e0d514fa7e9f",
   "metadata": {
    "id": "b0e2a31a-0e26-4747-aec2-e0d514fa7e9f"
   },
   "source": [
    "In fact, the TensorFlow function `tf.nn.embedding_lookup` is not so different.\n",
    "- The embedding table should be a **trainable variable**, rather than a simple NumPy Array.\n",
    "- The embedding table needs better initialization.\n",
    "    - so that embedding vectors end up with some kind of semantic meaning and grammatical role of each word after the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243ecc1-9c5f-4015-839e-d4c6c2cecf08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1656613552802,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "8243ecc1-9c5f-4015-839e-d4c6c2cecf08",
    "outputId": "a94d384f-801d-41dc-b2da-c864494bf0e6"
   },
   "outputs": [],
   "source": [
    "# tf_embedding_table = tf.Variable(tf.random.normal([9, 2], stddev=0.01, dtype=tf.float32))\n",
    "# tf_embedding_vectors = tf.nn.embedding_lookup(tf_embedding_table, X_trigram)\n",
    "# tf_embedding_vectors = tf.reshape(tf_embedding_vectors, (12, 2*2)) \n",
    "# # (12 input data in a batch, 2 words in a single input x 2 dimensions for each word)\n",
    "\n",
    "# ## If the reshaping is confusing to you, \n",
    "# ##     you can un-comment the following block and double-check\n",
    "# # tf_embedding_vector1 = tf.nn.embedding_lookup(tf_embedding_table, X_trigram[:, 0])\n",
    "# # tf_embedding_vector2 = tf.nn.embedding_lookup(tf_embedding_table, X_trigram[:, 1])\n",
    "# # tf_embedding_vectors_check = tf.concat((tf_embedding_vector1, tf_embedding_vector2), axis=1)\n",
    "# # print(tf_embedding_vectors == tf_embedding_vectors_check)\n",
    "\n",
    "# print(tf_embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79525f24",
   "metadata": {},
   "source": [
    "On the topic, you are also free to use the layered version, [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). Feel free to use it if it helps simplify your implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568b749-aab0-477f-aaf5-adbe1a70ca8a",
   "metadata": {
    "id": "7568b749-aab0-477f-aaf5-adbe1a70ca8a"
   },
   "source": [
    "### Train and Test Trigram\n",
    "\n",
    "Now you should be ready to complete every function in the file `trigram.py`. Go finish `trigram.py` and come back when you're ready!\n",
    "\n",
    "Steps to take: \n",
    "- Load the data with `get_data`\n",
    "- Reshape the input and output data into the Trigram shape\n",
    "- Initialize the model, train it, and calculate the perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1082bc-c75d-4a6e-b29d-a9fd2c27f342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620979,
     "status": "ok",
     "timestamp": 1656614173780,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "7f1082bc-c75d-4a6e-b29d-a9fd2c27f342",
    "outputId": "43bbe216-1aa7-4a74-d3be-a7d57fefc884"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import trigram \n",
    "\n",
    "# ## Load the data (from trigram.py)\n",
    "# train_id, test_id, vocab = preprocess.get_data(f\"{data_path}/train.txt\", f\"{data_path}/test.txt\")\n",
    "\n",
    "# ## Process the data\n",
    "# def process_trigram_data(data):\n",
    "#     X = np.array(data[:-1])\n",
    "#     Y = np.array(data[2:])\n",
    "#     X = np.column_stack((X[:-1], X[1:]))\n",
    "#     return X, Y\n",
    "\n",
    "# X0, Y0 = process_trigram_data(train_id)\n",
    "# X1, Y1 = process_trigram_data(test_id)\n",
    "\n",
    "# ## Feel free to paste your implementation in here if you want.\n",
    "# trigram_args = trigram.get_text_model(vocab)\n",
    "\n",
    "# trigram_args.model.fit(\n",
    "#     X0, Y0,\n",
    "#     epochs=trigram_args.epochs, \n",
    "#     batch_size=trigram_args.batch_size,\n",
    "#     validation_data=(X1, Y1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5919412-d77e-44e3-b81a-9cfa4fe4ee0b",
   "metadata": {
    "id": "a5919412-d77e-44e3-b81a-9cfa4fe4ee0b"
   },
   "source": [
    "### Generate Sentences with Trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707fd43e-8d65-4edb-8e8e-6312b1db40b0",
   "metadata": {
    "id": "707fd43e-8d65-4edb-8e8e-6312b1db40b0"
   },
   "source": [
    "Try the model with your own pairs of starting words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d68730-6177-4c97-8a7b-240068105319",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1656614174188,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "24d68730-6177-4c97-8a7b-240068105319",
    "outputId": "92f99f83-0422-4c7e-cb9a-6b9faa82337a"
   },
   "outputs": [],
   "source": [
    "# starting_words = [(\"computer\", \"is\"), (\"i\", \"am\"), (\"something\", \"new\"), (\"this\", \"is\")]\n",
    "\n",
    "# for first, second in starting_words:\n",
    "#     trigram_args.model.generate_sentence(first, second, 45, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e282e8",
   "metadata": {},
   "source": [
    "See anything interesting? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbe364-798d-4fac-b23d-af05b1e58373",
   "metadata": {
    "id": "defbe364-798d-4fac-b23d-af05b1e58373"
   },
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba73a2-33bb-4ec0-8f46-b9c0ceb57c09",
   "metadata": {
    "id": "adba73a2-33bb-4ec0-8f46-b9c0ceb57c09"
   },
   "source": [
    "### RNN Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7578cd2-094c-4264-8bcb-aa724bd72640",
   "metadata": {
    "id": "b7578cd2-094c-4264-8bcb-aa724bd72640"
   },
   "source": [
    "To train an RNN model, the input and output needs to be shaped differently than the trigram model. For an RNN model, the words need to be divided into non-overlapping windows of a fixed size. \n",
    "- **Window size** means the number of words each window contains.\n",
    "- **Non-overlapping** means that a window and the next window do not share a word\n",
    "\n",
    "Let's do a toy example again with our example sentence\n",
    " - `['the', 'word', '<_unk>', 'is', 'not', 'a', 'common', 'word', 'but', 'flower', 'is', 'a', 'common', 'word']`\n",
    "\n",
    "With our example sentence, one possible arangement of the windows for the input `X` and the output `y` can be the following. \n",
    "\n",
    "```\n",
    "X = [['the', 'word', '<_unk>', 'is'],\n",
    "     ['not', 'a', 'common', 'word'],\n",
    "     ['but', 'flower', 'is', 'a']]\n",
    "     \n",
    "y = [['word', '<_unk>', 'is', 'not'],\n",
    "     ['a', 'common', 'word', 'but'],\n",
    "     ['flower', 'is', 'a', 'common']]\n",
    "```\n",
    "\n",
    "In this example, \n",
    "+ We have divided the example sentence into non-overlapping windows of size 4. \n",
    "\n",
    "- Notice that there is no overlapping word between two consecutives windows.\n",
    "   - For example `['the', 'word', '<_unk>', 'is']` and `['not', 'a', 'common', 'word']`. \n",
    "   \n",
    "+ The last word in the example sentence `\"word\"` ended up not being included in the input and output data. \n",
    "  + That happened because 13 divided by 4 has one remainder.\n",
    "    + 13 is the length of the example sentence **minus one**.\n",
    "    + 4 is the window size.\n",
    "  + **Why minus one** and not the length of the example sentence itself? \n",
    "    + **The output windows in `y` have to be exactly one word behind of their corresponding input windows in `X`.**\n",
    "    \n",
    "|Index| Input `X`                        | Output `y`                       |\n",
    "|-----|----------------------------------|----------------------------------|\n",
    "|  0  |\\['the', 'word', '<_unk>', 'is'\\] | \\['word', '<_unk>', 'is', 'not'\\]|\n",
    "|  1  |\\['not', 'a', 'common', 'word'\\]  | \\['a', 'common', 'word', 'but'\\] |\n",
    "|  2  |\\['but', 'flower', 'is', 'a'\\]    | \\['flower', 'is', 'a', 'common'\\]|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b0145-32da-40db-827c-a238d6c7a092",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1656614174189,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "ce9b0145-32da-40db-827c-a238d6c7a092",
    "outputId": "99ac0943-a303-470d-894e-7b43b123b545"
   },
   "outputs": [],
   "source": [
    "print(f\"token list = {example_sentence_tokenized}\")\n",
    "window_size = 4\n",
    "\n",
    "# 1. Do some NumPy manipulation on the example sentence \n",
    "example_sentence_tokenized_array = np.array(example_sentence_tokenized)\n",
    "remainder = (len(example_sentence_tokenized_array) - 1)%window_size\n",
    "example_sentence_tokenized_array = example_sentence_tokenized_array[:-remainder]\n",
    "\n",
    "# 2. Define X_RNN and y_RNN  \n",
    "X_RNN = example_sentence_tokenized_array[:-1].reshape(-1, 4)\n",
    "y_RNN = example_sentence_tokenized_array[1:].reshape(-1, 4)\n",
    "\n",
    "print(f\"X_RNN shape = {X_RNN.shape}\")\n",
    "print(f\"y_RNN shape = {y_RNN.shape}\")\n",
    "\n",
    "print(f\"X_RNN     --> y_RNN\")\n",
    "for each_X, each_y in zip(X_RNN, y_RNN):\n",
    "    print(f\"{each_X} --> {each_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70d760-1852-45a5-bb0b-8d6f70b80b47",
   "metadata": {
    "id": "1f70d760-1852-45a5-bb0b-8d6f70b80b47"
   },
   "source": [
    "The result should look like this\n",
    "```\n",
    "token list = [7, 8, 0, 5, 6, 1, 3, 8, 2, 4, 5, 1, 3, 8]\n",
    "X_RNN shape = (3, 4)\n",
    "y_RNN shape = (3, 4)\n",
    "X_RNN     --> y_RNN\n",
    "[7 8 0 5] --> [8 0 5 6]\n",
    "[6 1 3 8] --> [1 3 8 2]\n",
    "[2 4 5 1] --> [4 5 1 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23fb186-7062-46d0-abb8-ec5bc99fe24c",
   "metadata": {
    "id": "c23fb186-7062-46d0-abb8-ec5bc99fe24c"
   },
   "source": [
    "PS. Have you noticed that we could have dropped the first word in the example sentence instead of the last word?\n",
    "- That's only because we did not offset the example sentence and wanted to keep as many words in the input and output\n",
    "- In practice we draw a **random integer between 0 and the (window size-1)** for every epoch\n",
    "- Then we remove the random integer number of words from the beginning of the training corpus (= offset)\n",
    "- Depending on the value of the random integer,\n",
    "  - Different words end up not being included in `X` and `y` from one epoch to another\n",
    "  - `X` and `y` end up having different number of windows from one epoch to another\n",
    "  - For every epoch, the model is trained with similar but slightly different sets of windows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af301afd-61a1-46f1-bd1e-4e7ad6d77822",
   "metadata": {
    "id": "af301afd-61a1-46f1-bd1e-4e7ad6d77822"
   },
   "source": [
    "### Keras LSTM Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa85c2-be08-4cef-ae14-ba20637987f5",
   "metadata": {
    "id": "a5aa85c2-be08-4cef-ae14-ba20637987f5"
   },
   "source": [
    "Let's take a close look at `tf.keras.layers.LSTM`'s API. First, the Keras LSTM Layer expects the input shape to be in the **batch-major form**, which means the dimensions of the input tensor should be in the order of `[batch, timesteps, embedding]`. \n",
    "\n",
    "- In our language model, `timesteps` is basically our `window`. \n",
    "  - That's because we treat a sequence of words as a time-series data.\n",
    "- It also has a keyword argument to enable the input in the **time-major form** `[timesteps, batch, embedding]`, \n",
    "  - but we can ignore that now, \n",
    "  - since all our data is already in the batch-major form anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf618442-b3fd-4dd9-8a22-1ac1a34c8465",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1656614174189,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "bf618442-b3fd-4dd9-8a22-1ac1a34c8465",
    "outputId": "fc20d306-43b5-42e1-c9f6-aac1cb244671"
   },
   "outputs": [],
   "source": [
    "embedding_size = 2\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=9, output_dim=embedding_size)\n",
    "X_RNN_embedding = embedding_layer(X_RNN)\n",
    "\n",
    "RNN_input_dim_order = [\"Batch\", \"Window\", \"Embedding\"]\n",
    "print(f\"RNN input tokens shape = {X_RNN.shape}\")\n",
    "print(f\"RNN embeddings shape   = {X_RNN_embedding.shape}\")\n",
    "for each_dim_meaning, each_dim in zip(RNN_input_dim_order, X_RNN_embedding.shape):\n",
    "    print(f\"{each_dim_meaning:>9s} Size = {each_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a2cc9-646d-4952-a162-2fd14aaa436d",
   "metadata": {
    "id": "cc1a2cc9-646d-4952-a162-2fd14aaa436d"
   },
   "source": [
    "Perhaps the most important keywards arguments are `units`, `return_state` and `return_sequences`. \n",
    "- `units`: the output embedding size\n",
    "  - It does not necessarily have to be the same as the input embedding size,\n",
    "  - but we will keep them same in this homework assignment. \n",
    "- `return_state`: the layer returns the last state for every window in the batch.\n",
    "- `return_sequences`: the layer returns the whole output sequence instead of the last output.\n",
    "\n",
    "Also please note that all Keras LSTM layers have the same weight structures, no matter the value of the Boolean flags.\n",
    "+ So, we can copy and paste the weights with the `.get_weights()` and `.set_weights()` methods.\n",
    "+ The input shapes are needed to initialize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422f84f-4877-4b2a-b687-17de9bc2fa45",
   "metadata": {
    "id": "a422f84f-4877-4b2a-b687-17de9bc2fa45"
   },
   "outputs": [],
   "source": [
    "batch_size, window_size, embedding_size= X_RNN_embedding.shape ## (3, 4, 2)\n",
    "units = embedding_size\n",
    "\n",
    "lstm           = tf.keras.layers.LSTM(units=embedding_size, return_sequences=False, return_state=False)\n",
    "lstm_state     = tf.keras.layers.LSTM(units=embedding_size, return_sequences=False, return_state=True )\n",
    "lstm_seq       = tf.keras.layers.LSTM(units=embedding_size, return_sequences=True,  return_state=False)\n",
    "lstm_seq_state = tf.keras.layers.LSTM(units=embedding_size, return_sequences=True,  return_state=True )\n",
    "\n",
    "lstm.build(X_RNN_embedding.shape)\n",
    "lstm_state.build(X_RNN_embedding.shape)\n",
    "lstm_seq.build(X_RNN_embedding.shape)\n",
    "lstm_seq_state.build(X_RNN_embedding.shape)\n",
    "\n",
    "lstm_weights = lstm.get_weights()\n",
    "lstm_state.set_weights(lstm_weights)\n",
    "lstm_seq.set_weights(lstm_weights)\n",
    "lstm_seq_state.set_weights(lstm_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e731b-2da8-4b61-afd1-492679c0281f",
   "metadata": {
    "id": "ff5e731b-2da8-4b61-afd1-492679c0281f"
   },
   "source": [
    "#### LSTM default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d148e5b-f496-460d-834c-9aebcac9a61f",
   "metadata": {
    "id": "5d148e5b-f496-460d-834c-9aebcac9a61f"
   },
   "source": [
    "The default output shape is `[batch, embedding]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957e936-aa4b-4191-bc3a-50c8421b263f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1656614174189,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "5957e936-aa4b-4191-bc3a-50c8421b263f",
    "outputId": "b5c5dc5a-92c7-4a35-f48c-5d09e6e49ec5"
   },
   "outputs": [],
   "source": [
    "output = lstm(X_RNN_embedding, initial_state = None)\n",
    "print(f\"default output, shape = {output.shape} \\n{output.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9504f-97b7-40cc-b9ab-538b1e9915bd",
   "metadata": {
    "id": "35f9504f-97b7-40cc-b9ab-538b1e9915bd"
   },
   "source": [
    "#### LSTM state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b6ee6-c2b0-4377-a6c1-81783c3eb1b9",
   "metadata": {
    "id": "e39b6ee6-c2b0-4377-a6c1-81783c3eb1b9"
   },
   "source": [
    "When `return_state` is `True`, it returns the last hidden state and the last cell state. \n",
    "- The output shape is still `[batch, embedding]`.\n",
    "- Notice that the last hidden state is the same as the output. \n",
    "- The hidden and cell states are in the shape `[batch, embedding]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460376d-3c87-4745-b59c-e9df398a5424",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1656614174468,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "e460376d-3c87-4745-b59c-e9df398a5424",
    "outputId": "c9748a0c-c13c-41ac-cab1-9f49b5a1aafe"
   },
   "outputs": [],
   "source": [
    "output, state_h, state_c = lstm_state(X_RNN_embedding, initial_state = None)\n",
    "\n",
    "print(f\"output, shape = {output.shape} \\n{output}\\n\")\n",
    "print(f\"state_h, shape = {state_h.shape} \\n{state_h}\\n\")\n",
    "print(f\"state_c, shape = {state_c.shape} \\n{state_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17676c7c-23f5-4970-9140-a114d578592d",
   "metadata": {
    "id": "17676c7c-23f5-4970-9140-a114d578592d"
   },
   "source": [
    "#### LSTM sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4725ef-9e1a-4af3-929b-960a703e757d",
   "metadata": {
    "id": "ef4725ef-9e1a-4af3-929b-960a703e757d"
   },
   "source": [
    "When `return_sequences` is `True`, it returns whole sequence of outputs. \n",
    "- The output sequence shape is `[batch, timesteps, embedding]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9ce59-c014-4894-910c-205c85ee59a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1656614174469,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "36b9ce59-c014-4894-910c-205c85ee59a0",
    "outputId": "983e97dc-bc9f-49a6-fa36-306a1ac96c7e"
   },
   "outputs": [],
   "source": [
    "output_seq = lstm_seq(X_RNN_embedding, initial_state = None)\n",
    "print(f\"output sequences, shape = {output_seq.shape} \\n{output_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ed4fc-e40e-4a1a-9ecf-554923532251",
   "metadata": {
    "id": "f55ed4fc-e40e-4a1a-9ecf-554923532251"
   },
   "source": [
    "#### LSTM state and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3743d-cd4d-4b64-b436-ce6ee2bcca6f",
   "metadata": {
    "id": "ebf3743d-cd4d-4b64-b436-ce6ee2bcca6f"
   },
   "source": [
    "When `return_states` and `return_sequences` are both `True`, it returns the last states and the whole sequence of outputs. \n",
    "- The output sequence shape is `[batch, timesteps, embedding]`.\n",
    "- **It does not return the sequences of states**. \n",
    "  - Unfortunately, there is no argument like `return_sequences_of_states`.\n",
    "  - When you think about it, the sequence of whole outputs are actually the sequence of whole hidden states,\n",
    "  - but we still do not know the sequence of whole cell states.  \n",
    "- It still returns the last states for every window in the batch.\n",
    "- The hidden and cell states are still in the shape `[batch, embedding]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4684fc4-45c1-492a-9648-b73b99b6aee5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1656614174469,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "d4684fc4-45c1-492a-9648-b73b99b6aee5",
    "outputId": "0f4e2fdd-c957-434e-cbc8-5c28f7180a01"
   },
   "outputs": [],
   "source": [
    "output_seq, state_h, state_c = lstm_seq_state(X_RNN_embedding, initial_state = None)\n",
    "\n",
    "print(f\"output sequences, shape = {output_seq.shape} \\n{output_seq}\\n\")\n",
    "print(f\"state_h, shape = {state_h.shape} \\n{state_h}\\n\")\n",
    "print(f\"state_c, shape = {state_c.shape} \\n{state_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063daaf9-6265-4619-b73f-16d02061c411",
   "metadata": {
    "id": "063daaf9-6265-4619-b73f-16d02061c411"
   },
   "source": [
    "### Keras GRU Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d867a5-51cf-4fb6-89e3-0529908e2906",
   "metadata": {
    "id": "b8d867a5-51cf-4fb6-89e3-0529908e2906"
   },
   "source": [
    "We can also take a look at `tf.keras.layers.GRU`. It is very similar to the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d7318-6327-4d50-b6a7-eaf47b2fe9cd",
   "metadata": {
    "id": "1f6d7318-6327-4d50-b6a7-eaf47b2fe9cd"
   },
   "outputs": [],
   "source": [
    "gru           = tf.keras.layers.GRU(units=embedding_size, return_sequences=False, return_state=False)\n",
    "gru_state     = tf.keras.layers.GRU(units=embedding_size, return_sequences=False, return_state=True )\n",
    "gru_seq       = tf.keras.layers.GRU(units=embedding_size, return_sequences=True,  return_state=False)\n",
    "gru_seq_state = tf.keras.layers.GRU(units=embedding_size, return_sequences=True,  return_state=True )\n",
    "\n",
    "# the Keras GRU layers initialize their weight \n",
    "#   not when they are declared\n",
    "#   but when they are complied\n",
    "gru.build(X_RNN_embedding.shape)\n",
    "gru_state.build(X_RNN_embedding.shape)\n",
    "gru_seq.build(X_RNN_embedding.shape)\n",
    "gru_seq_state.build(X_RNN_embedding.shape)\n",
    "\n",
    "# Now all four layers have exact same weights\n",
    "gru_weights = gru.get_weights()\n",
    "gru_state.set_weights(gru_weights)\n",
    "gru_seq.set_weights(gru_weights)\n",
    "gru_seq_state.set_weights(gru_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5aece-9ec3-4830-8730-d97c4ba83623",
   "metadata": {
    "id": "dac5aece-9ec3-4830-8730-d97c4ba83623"
   },
   "source": [
    "#### GRU default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d2aeb-4d33-486d-8814-588a18177f81",
   "metadata": {
    "id": "6c6d2aeb-4d33-486d-8814-588a18177f81"
   },
   "source": [
    "The default output shape is `[batch, embedding]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a8ed7-c769-4e1e-b764-dd5eac1d937e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656614174469,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "306a8ed7-c769-4e1e-b764-dd5eac1d937e",
    "outputId": "44d376c6-bdf5-4ab2-c257-7f507eef9bcb"
   },
   "outputs": [],
   "source": [
    "output = gru(X_RNN_embedding, initial_state = None)\n",
    "print(f\"default output, shape = {output.shape} \\n{output.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5d499-f5da-4e16-9126-c53b5a43a094",
   "metadata": {
    "id": "abc5d499-f5da-4e16-9126-c53b5a43a094"
   },
   "source": [
    "#### GRU state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11285b-de84-4375-bede-0d0ae78d2582",
   "metadata": {
    "id": "cf11285b-de84-4375-bede-0d0ae78d2582"
   },
   "source": [
    "When `return_state` is `True`, it returns the last hidden state and the last cell state. \n",
    "- The output shape is still `[batch, embedding]`.\n",
    "- Notice that the last hidden state is the same as the output. \n",
    "- The hidden and cell states are in the shape `[batch, embedding]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb66328-fd42-4fa6-ab2e-c520934519d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656614174470,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "1fb66328-fd42-4fa6-ab2e-c520934519d5",
    "outputId": "a3ac2cf7-caa5-42da-a1a3-c0d7f2d985fc"
   },
   "outputs": [],
   "source": [
    "output, state_gru = gru_state(X_RNN_embedding, initial_state = None)\n",
    "\n",
    "print(f\"output, shape = {output.shape} \\n{output}\\n\")\n",
    "print(f\"state_gru, shape = {state_gru.shape} \\n{state_gru}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156ac44-2f73-4a2c-acbb-5a9b196358e3",
   "metadata": {
    "id": "1156ac44-2f73-4a2c-acbb-5a9b196358e3"
   },
   "source": [
    "#### GRU sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97379169-ada2-4fbb-a94e-9f46602bbe06",
   "metadata": {
    "id": "97379169-ada2-4fbb-a94e-9f46602bbe06"
   },
   "source": [
    "When `return_sequences` is `True`, it returns whole sequence of outputs. \n",
    "- The output sequence shape is `[batch, timesteps, embedding]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c873e-aa80-45ec-b357-ae6cdf35a231",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1656614174650,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "030c873e-aa80-45ec-b357-ae6cdf35a231",
    "outputId": "5dd07d23-1525-49d5-a3d5-d5f6c8a8d2e7"
   },
   "outputs": [],
   "source": [
    "output_seq = gru_seq(X_RNN_embedding, initial_state = None)\n",
    "print(f\"output sequences, shape = {output_seq.shape} \\n{output_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e26956-b383-4ba4-a5bf-5ffedb0fa6db",
   "metadata": {
    "id": "c6e26956-b383-4ba4-a5bf-5ffedb0fa6db"
   },
   "source": [
    "#### GRU state and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0ead0-39b2-4c7e-abc4-69ea0248b326",
   "metadata": {
    "id": "3dc0ead0-39b2-4c7e-abc4-69ea0248b326"
   },
   "source": [
    "When `return_states` and `return_sequences` are both `True`, it returns the last states and the whole sequence of outputs. \n",
    "- The output sequence shape is `[batch, timesteps, embedding]`.\n",
    "- It does not return the sequences of states again. \n",
    "  - **However**, the sequence of whole outputs are actually the sequence of whole hidden states,\n",
    "- It still returns the last states for every window in the batch.\n",
    "- The hidden and cell states are still in the shape `[batch, embedding]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990c86f-f642-4950-a1da-c6e9ee2ab9d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1656614174650,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "5990c86f-f642-4950-a1da-c6e9ee2ab9d8",
    "outputId": "a951f978-2c86-4d95-e42d-de37761ffa64"
   },
   "outputs": [],
   "source": [
    "output_seq, state_gru = gru_seq_state(X_RNN_embedding, initial_state = None)\n",
    "\n",
    "print(f\"output sequences, shape = {output_seq.shape} \\n{output_seq}\\n\")\n",
    "print(f\"state_gru, shape = {state_gru.shape} \\n{state_gru}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc0ae0-cec4-42cf-86ba-cf9c17cfcb6d",
   "metadata": {
    "id": "9fdc0ae0-cec4-42cf-86ba-cf9c17cfcb6d"
   },
   "source": [
    "### Train and Test with RNN\n",
    "\n",
    "Now you should be ready to complete every function in the file `trigram.py`. Go finish `trigram.py` and come back to here.\n",
    "\n",
    "Steps to take: \n",
    "- Load the data with `get_data`\n",
    "- Reshape the input and output data into the RNN shape\n",
    "- Initialize the model, train it, and calculate the perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dd1fcec-21ed-497c-a1f3-b26255ed65c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539706,
     "status": "ok",
     "timestamp": 1656614714354,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "7dd1fcec-21ed-497c-a1f3-b26255ed65c6",
    "outputId": "ea87bc66-e79d-4218-9304-c053578cb260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 620s 915ms/step - loss: 10.9135 - perplexity: 58024.1484 - val_loss: 9.7583 - val_perplexity: 18812.4570\n",
      "CPU times: total: 1h 5min 48s\n",
      "Wall time: 15min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114dd40dfa0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import rnn\n",
    "\n",
    "train_tracks_id, test_tracks_id, track_to_id, relevance_output = preprocessing.preprocess()\n",
    "\n",
    "train_id = np.array(train_id)\n",
    "test_id  = np.array(test_id)\n",
    "X0, Y0 = train_id[:-1], train_id[1:]\n",
    "X1, Y1  = test_id[:-1],  test_id[1:]\n",
    "\n",
    "\n",
    "X0 = X0[:-(len(X0) % 20)]\n",
    "Y0 = Y0[:-(len(Y0) % 20)]\n",
    "X1 = X1[:-(len(X1) % 20)]\n",
    "Y1 = Y1[:-(len(Y1) % 20)]\n",
    "\n",
    "np.reshape(X0, (-1, 20))\n",
    "np.reshape(X1, (-1, 20))\n",
    "np.reshape(Y0, (-1, 20))\n",
    "np.reshape(Y1, (-1, 20))\n",
    "\n",
    "rnn_args = rnn.get_text_model(train_tracks_id)\n",
    "\n",
    "rnn_args.model.fit(\n",
    "    X0, Y0,\n",
    "    epochs=rnn_args.epochs,\n",
    "    batch_size=rnn_args.batch_size,\n",
    "    validation_data=(X1, Y1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ef199-d1c1-465c-b629-9d3d8cf8e1f1",
   "metadata": {
    "id": "1a8ef199-d1c1-465c-b629-9d3d8cf8e1f1"
   },
   "source": [
    "### Generate Sentences with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93c257-6c85-494d-9bb3-fdd28b5fe403",
   "metadata": {
    "id": "dc93c257-6c85-494d-9bb3-fdd28b5fe403"
   },
   "source": [
    "Try the model with your own pairs of starting words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8de51a2b-44e0-44c5-88ad-06cbd1d9e1d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1656614714980,
     "user": {
      "displayName": "Yeunun Choo",
      "userId": "09529988632388209490"
     },
     "user_tz": 240
    },
    "id": "8de51a2b-44e0-44c5-88ad-06cbd1d9e1d0",
    "outputId": "30bae9cb-9be5-4da5-cdbf-e0b51943616d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say It Ain't So, Gold, Caroline, Ride, Waves, Congratulations, Congratulations, Congratulations, HUMBLE., Closer, Don't Let Me Down, Go Flex, Don't Let Me Down, Congratulations, Don't Let Me Down, Don't Let Me Down, Congratulations, Don't Let Me Down, Gold Digger, Ni**as In Paris, Congratulations\n",
      "\n",
      "Island In The Sun, Gold Digger, Ride, One Dance, Don't Let Me Down, HUMBLE., Closer, Gold, Let Me Love You, Closer, Forever, Congratulations, Closer, Congratulations, Party In The U.S.A., Closer, Caroline, Gold, Let Me Love You, Gold, Caroline\n",
      "\n",
      "Undone - The Sweater Song, Let Me Love You, Sorry, Let Me Love You, One Dance, Jungle, One Dance, Bodak Yellow, Gold, Skinny Love, Congratulations, Closer, Caroline, Don't Let Me Down, Mask Off, Don't Let Me Down, Don't Let Me Down, Mask Off, Don't Let Me Down, Go Flex, Don't Let Me Down\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Feel free to mess around with the word list to see the model try to generate sentences\n",
    "for word1 in [\"Say It Ain't So\", \"Island In The Sun\", \"Undone - The Sweater Song\"]:\n",
    "    if word1 not in vocab: print(f\"{word1} not in vocabulary\")            \n",
    "    else: rnn_args.model.generate_sentence(word1, 20, vocab, 10)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_LM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('eric')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "187148e4c5ed58496f5273f307f082177127925a573279df7cfa977aab41e962"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
